{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recitation 13.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anitayea/Linear_and_Nonlinear_Optimization/blob/recitations/Recitation_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recitation 11\n",
        "\n",
        "\n",
        "\n",
        "*   Quiz 11\n",
        "*   HW 5\n",
        "\n"
      ],
      "metadata": {
        "id": "IwAcGW7xrmdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz 11\n",
        "\n",
        "\n",
        "1) The Quasi-Newton rank-one update seen in class:\n",
        "\n",
        "(i) preserves positive definitedness of $H_{k}$, and ensures $%\n",
        "p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq k$.\n",
        "\n",
        "(i) preserves positive definitedness of $H_{k}$, but does not necessarily\n",
        "ensure $p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq k$.\n",
        "\n",
        "(iii) does not necessarily preserves positive definitedness of $H_{k}$, but\n",
        "ensures $p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq k$.\n",
        "\n",
        "(iv) does not necessarily preserves positive definitedness of $H_{k}$ and\n",
        "does not necessarily ensure $p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq k$.\n",
        "\n",
        "Answer. Statement (iii): does not necessarily preserves positive\n",
        "definitedness of $H_{k}$, but ensures $p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq\n",
        "k $.\n"
      ],
      "metadata": {
        "id": "CcKawsdar08h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2) In the quadratic case, the DFP Quasi-Newton method seen in class\n",
        "satisfies\n",
        "\\begin{equation*}\n",
        "p_{i}^{\\top }Fp_{j}=0\\text{ for }0\\leq i<j\\leq k.\n",
        "\\end{equation*}\n",
        "(the notations are those from the lecture notes).\n",
        "\n",
        "Answer: True"
      ],
      "metadata": {
        "id": "REdQQO9FsOWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) The BFGS method is a rank 2 update method.\n",
        "\n",
        "Answer: True"
      ],
      "metadata": {
        "id": "wixLRd29sTbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4) Consider the conjugate gradient descent method to solve $x^{\\ast\n",
        "}=Q^{-1}b $. Then the gradients $g_{k}=Qx_{k}-b$ are $Q$-orthogonal.\n",
        "\n",
        "Answer: False - the $d_i$'s are orthogonal (descent directions)"
      ],
      "metadata": {
        "id": "6K0RpNP2sU6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) In the quadratic case, the conjugate descent method converges in one step.\n",
        "\n",
        "Answer: False - converges in n steps"
      ],
      "metadata": {
        "id": "y8LYc0-8sWWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "6) Consider the problem\n",
        "\\begin{equation*}\n",
        "\\min_{x\\in \\left[ 0,1\\right] }f\\left( x\\right) \n",
        "\\end{equation*}\n",
        "where $f$ is a continuous function. Then the function $P\\left( x\\right)\n",
        "=1\\left\\{ x\\notin \\left[ 0,1\\right] \\right\\} $ is a penalty function for the\n",
        "problem.\n",
        "\n",
        "Answer: False - it is not continous and penalty functions need to be continous\n"
      ],
      "metadata": {
        "id": "n75e_ABXsXl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 5\n",
        "\n",
        "1. (i)\n",
        "\n",
        "Start with the definition of lipschitz that is given, and plug in $x = x+\\epsilon v$ and $x' = x$ where\n",
        "\n",
        "\\begin{align}\n",
        "  |\\nabla f(x+\\epsilon v) - \\nabla f(x)| \\leq \\beta \\epsilon v\\\\\n",
        "  |\\frac{\\nabla f(x+\\epsilon v) - \\nabla f(x)}{\\epsilon}| \\leq \\beta |v|\n",
        "\\end{align}\n",
        "\n",
        "then just take the limit as $\\epsilon \\to 0$\n",
        "$|\\nabla^2 f(x)v| \\leq \\beta |v|$\n",
        "\n",
        "thus \n",
        "\n",
        "$|v^T\\nabla^2 f(x)v| \\leq \\beta |v|^2$\n",
        "\n",
        "(ii)\n",
        "$y = x+ \\epsilon z$\n",
        "\n",
        "$\\nabla f(x) - \\nabla f(y) \\approx \\epsilon\\nabla^2 f(x)^T z$ \n",
        "\n",
        "\n",
        "let $\\phi(t) = \\nabla f(x + t(y-x))$ this is $C^1$ and \n",
        "\n",
        "$\\phi(1) - \\phi(0) \\leq \\sup_{t\\in[0,1]}|\\phi'(t)|$\n",
        "\n",
        "$|\\nabla f(x) - \\nabla f(y)| \\leq |\\int_0^1 \\nabla^2 f(x+t(y-x))^T(y-x)|dt \\leq \\beta |y-x|$\n",
        "\n",
        "(iii)\n",
        "done in class, add inequality for $x$ and $x'$ with inequality for $x'$ and $x$ then rearrange and use Cauchy-Schwartz"
      ],
      "metadata": {
        "id": "V12Jq6zmtsHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. \n",
        "(i)\n",
        "\n",
        "\\begin{align}\n",
        "<\\nabla h(x) - \\nabla h(x'), x-x'>&= < \\beta x - \\nabla f(x) - \\beta x' + \\nabla f(x'), x-x' >\\\\\n",
        "&\\geq \\beta |x-x'| - |\\nabla f(x) - \\nabla f(x')||x-x'|\\\\\n",
        "&\\geq 0\n",
        "\\end{align}\n",
        "\n",
        "(ii)\n",
        "\n",
        "$\\phi (t) = h(x + t(x'-x))$\n",
        "\n",
        "Want to show $\\phi'(t) \\geq \\phi'(0)$ fot t>0\n",
        "\n",
        "\\begin{align}\n",
        "\\phi'(t) &= \\nabla h(x+t(x'-x))^T(x'-x)\\\\\n",
        "\\phi'(0) &= \\nabla h(x)^T(x'-x)\\\\\n",
        "\\phi'(t) - \\phi'(0) &= \\Big(\\nabla h(x+t(x'-x)) - \\nabla h(x)\\Big)^T(x'-x)\\\\\n",
        "&= \\Big(\\nabla h(x) - \\nabla h(x+t(x'-x))\\Big)^T(x-x')\\\\\n",
        "&\\text{let } y = x+t(x'-x)\\\\\n",
        "&= \\Big(\\nabla h(x) - \\nabla h(y)\\Big)^T\\Big(x-\\big(\\frac{y-x}{t}+x\\big)\\Big)\\\\\n",
        "&= \\frac{1}{t}\\Big(\\nabla h(x) - \\nabla h(y)\\Big)^T\\Big(x-y)\\Big)\\\\\n",
        "&\\geq 0 \\quad \\text{by part (i)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Finally apply to the integral from 0 to 1 of $\\phi$ to get the following:\n",
        "\n",
        "$h(x') = h(x + (x'-x)) = \\phi(1) \\geq \\phi(0)+\\phi'(0) = h(x) + \\nabla h(x)^T(x'-x)$\n",
        "\n",
        "(iii)\n",
        "Note: there was a typo, it should state $j(z) = h(z) + \\nabla f(x)^Tz$ then from there we can use the inequality given and plug in $j$ then plug in $h$\n",
        "\n",
        "\\begin{align}\n",
        "j( z) \\geq j(\n",
        "y) +\\nabla j( y) ^{\\top }( z-y) \\\\\n",
        "h( z) +\\nabla f( x) ^{\\top }z &\\geq h( y)\n",
        "+\\nabla f( x) ^{\\top }y+( \\nabla h( y) +\\nabla\n",
        "f( x) ) ^{\\top }( z-y)\\\\\n",
        "\\beta \\frac{z^{2}}{2}-f( z) +\\nabla f( x) ^{\\top\n",
        "}z &\\geq \\beta \\frac{y^{2}}{2}+\\nabla f( x) ^{\\top }y-f(\n",
        "y) +( \\beta y+\\nabla f( x) -\\nabla f( y)\n",
        ") ( z-y)\\\\\n",
        "f( z) -\\nabla f( x) ^{\\top }z &\\leq \\beta \\frac{z^{2}\n",
        "}{2}-\\beta \\frac{y^{2}}{2}-\\nabla f( x) ^{\\top }y+f(\n",
        "y) +( \\beta y+\\nabla f( x) -\\nabla f( y)\n",
        ") ( y-z)\\\\\n",
        "&=f( y) -\\nabla f( x) ^{\\top }y+\\beta \\frac{\n",
        "\\vert z-y\\vert ^{2}}{2}+( \\nabla f( x) -\\nabla\n",
        "f( y) ) ^{\\top }( y-z)\\\\\n",
        "k_1(z) &\\leq k_2(z)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "(iv)\n",
        "\n",
        "$\\min k_1(z) = f(z) - \\nabla f(x)^T z$\n",
        "\n",
        "$\\nabla k_1(z) = \\nabla f(z) - \\nabla f(x) = 0 \\implies z=x$\n",
        "\n",
        "$\\min k_2(z) = f(y) - \\nabla f(x)^Ty + (\\nabla f(y) - \\nabla f(x))^T(z-y) + \\frac{\\beta}{2}||z-y||^2$\n",
        "\n",
        "$\\nabla k_2(z) = \\nabla f(y) - \\nabla f(x) + \\beta(z-y) = 0 \\implies \\beta z = \\beta y + \\nabla f(x) - \\nabla f(y)$\n",
        "\n",
        "$z = y + \\frac{\\nabla f(x) - \\nabla f(y)}{\\beta}$"
      ],
      "metadata": {
        "id": "lOfknt7yxZVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "&&\\left\\vert \\lambda _{1}\\nabla f_{1}\\left( x^{\\prime }\\right) +\\lambda\n",
        "_{2}\\nabla f_{2}\\left( x^{\\prime }\\right) -\\lambda _{1}\\nabla f_{1}\\left(\n",
        "x\\right) -\\lambda _{2}\\nabla f_{2}\\left( x\\right) \\right\\vert \\\\\n",
        "&\\leq &\\lambda _{1}\\left\\vert \\nabla f_{1}\\left( x^{\\prime }\\right) -\\nabla\n",
        "f_{1}\\left( x\\right) \\right\\vert +\\lambda _{2}\\left\\vert \\nabla f_{2}\\left(\n",
        "x^{\\prime }\\right) -\\nabla f_{2}\\left( x\\right) \\right\\vert \\\\\n",
        "&\\leq &\\left( \\lambda _{1}\\beta _{1}+\\lambda _{2}\\beta _{2}\\right)\n",
        "\\left\\vert x^{\\prime }-x\\right\\vert .\n",
        "\\end{eqnarray*}"
      ],
      "metadata": {
        "id": "M-YFGdjQ6XOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. \n",
        "\n",
        "(i) Problem is find $p$ such that $D(p) = S(p)$ or Equivently that $S(p) - D(p) = 0$ so if we find a convex $f$ such that $\\nabla f = S(p) - D(p)$ then minimizing $f$ gives us the optimal $p$\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "f\\left( p\\right) &=&10\\log \\left( 1+e^{3-p_{1}}+e^{5-p_{2}}\\right) +5\\log\n",
        "\\left( 1+e^{2-p_{1}}+e^{6-p_{2}}\\right) \\\\\n",
        "&&+7\\log \\left( 1+e^{5-p_{1}}+e^{4-p_{2}}\\right) +6\\log \\left(\n",
        "1+e^{p_{1}-3}+e^{p_{2}-4}\\right)\n",
        "\\end{eqnarray*}\n",
        "\n",
        "(ii) $\\nabla f$ is $\\beta$-Lipschitz with $\\beta = 10+5+7+6 = 28$\n",
        "\n",
        "Then run code from lecture 20 with different inputs to get answer which is $\\approx (5.7, 6.8)$\n",
        "\n"
      ],
      "metadata": {
        "id": "BYRD6K1h6fvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. \n",
        "\n",
        "$f(x) = e^{x_1} + e^{-x_2}$\n",
        "\n",
        "$x_1 \\to -\\infty$ and $x_2 \\to +\\infty$\n",
        "\n",
        "(i) 0\n",
        "\n",
        "(ii) no, because for any $x\\in R^n$ we can find another $x' \\in R^n$ that acheives a lower value of $f$\n",
        "\n",
        "(iii)\n",
        "\n",
        "Proof by contradiction. Assume that $\\nabla f(x^k) = (\\lambda_i e^{\\lambda_ix_i^k})_i$ does not converge.\n",
        "\n",
        "Then there is an entry $i$ such that $e^{\\lambda_ix_i^k} \\geq m$ for some $m$ and for all $k$ \n",
        "\n",
        "Then $\\lambda_ix_i^{k+1} > \\log m$ by $\\lambda_ix_i^{k+1} = \\lambda_ix_i^0 - \\sum_{l=0}^k \\alpha\\lambda_i^2e^{\\lambda_ix_i^l} \\leq \\lambda_ix_i^0 - (k+1)\\alpha \\lambda_i^2 m$ which tends to $-\\infty$ as $k\\to +\\infty$\n",
        "\n",
        "(iv) $x_i^k \\to -\\text{sign}(\\lambda_i)\\infty$ as $k\\to \\infty$\n",
        "\n"
      ],
      "metadata": {
        "id": "W_l2EpLn8m2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. \n",
        "\n",
        "(i)\n",
        "$R - \\{x_i^*\\}_{i=1}^n$\n",
        "\n",
        "(ii)\n",
        "$\\nabla f(x)_i = \\lambda_i(1_{\\{x_i > x_i^*\\}} - 1_{\\{x_i < x_i^*\\}})$ when $x_i \\not = x_i^*$ otherwise when $x_i = x_i^*$ the gradient doesn't exist.\n",
        "\n",
        "(iii)\n",
        "Because $g(x)_i$ is either $-\\lambda,0,$ or $\\lambda$ whenever we take step along the coordinate $x_i$ we take a step of length $\\alpha \\lambda_i$. So what if $x_i^0 - x_i^*$ isn't divisible by $\\alpha \\lambda_i$"
      ],
      "metadata": {
        "id": "CHXv5iddADTt"
      }
    }
  ]
}