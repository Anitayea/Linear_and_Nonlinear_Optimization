{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quizzes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNq2UT0tjtK6lvhWnF7UdI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anitayea/Linear_and_Nonlinear_Optimization/blob/main/Quizzes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kLsNsNYCMQF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz 11\n",
        "\n",
        "\n",
        "1) The Quasi-Newton rank-one update does not necessarily preserves positive definitedness of $H_{k}$, but\n",
        "ensures $p_{i}=H_{k+1}q_{i}$ for $0\\leq i\\leq k$.\n",
        "\n",
        "\\\n",
        "\n",
        "2) In the quadratic case, the DFP Quasi-Newton method seen in class\n",
        "satisfies\n",
        "\\begin{equation*}\n",
        "p_{i}^{\\top }Fp_{j}=0\\text{ for }0\\leq i<j\\leq k.\n",
        "\\end{equation*}\n",
        "(the notations are those from the lecture notes).\n",
        "\n",
        "Answer: True\n",
        "\n",
        "\\\n",
        "\n",
        "3) The BFGS method is a rank 2 update method.\n",
        "\n",
        "Answer: True\n",
        "\n",
        "\\\n",
        "\n",
        "4) Consider the conjugate gradient descent method to solve $x^{\\ast\n",
        "}=Q^{-1}b $. Then the gradients $g_{k}=Qx_{k}-b$ are $Q$-orthogonal.\n",
        "\n",
        "Answer: False - the $d_i$'s are orthogonal (descent directions)\n",
        "\n",
        "\\\n",
        "\n",
        "5) In the quadratic case, the conjugate descent method converges in one step.\n",
        "\n",
        "Answer: False - converges in n steps\n",
        "\n",
        "\\\n",
        "\n",
        "6) Consider the problem\n",
        "\\begin{equation*}\n",
        "\\min_{x\\in \\left[ 0,1\\right] }f\\left( x\\right) \n",
        "\\end{equation*}\n",
        "where $f$ is a continuous function. Then the function $P\\left( x\\right)\n",
        "=1\\left\\{ x\\notin \\left[ 0,1\\right] \\right\\} $ is a penalty function for the\n",
        "problem.\n",
        "\n",
        "Answer: False - it is not continous and penalty functions need to be continous\n"
      ],
      "metadata": {
        "id": "OZxUwpVjF_YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## quiz 10\n",
        "1. When the function to optimize $f$ is smooth and convex, and has $%\n",
        "\\beta $-Lipschitz gradient, the standard gradient descent method\n",
        "with an appropriate fixed step size is such that $f\\left(\n",
        "x^{k}\\right) -f\\left( x^{\\ast }\\right) $ converges to zero at rate: $1/k$\n",
        "\n",
        "2. When the function to optimize $f$ is smooth and convex, and has $%\n",
        "\\beta $-Lipschitz gradient, the accelerated gradient descent method\n",
        "with an appropriate fixed step size is such that $f\\left(\n",
        "x^{k}\\right) -f\\left( x^{\\ast }\\right) $ converges to zero at rate: $1/k^{2}$\n",
        "\n",
        "3. Consider Newton's method applied to a function $f$ with the\n",
        "\"right\" properties stated in class. Close\n",
        "to a minimum, the number of digits at which $x^{k}$ approximates $x^{\\ast }$\n",
        "doubles at each iteration.\n",
        "\n",
        "  ANSWER: True - quadratic convergence (which is superlinear) \n",
        "\n",
        "4. Even when $f$ is not convex, Newton's method always weakly decreases the value of $f\\left( x^{k}\\right) $.\n",
        "\n",
        "  ANSWER: False. if concave then increases, example $-x^2$\n",
        "\n",
        "5. When applied to a smooth convex function, Newton's descent method is guaranteed to converge to a local minimum from any starting point in the interior of the domain.\n",
        "\n",
        "  ANSWER: False - we are only gaurenteed converge if we start close enough to a local min. \n",
        "\n",
        "6. Stochastic gradient descent works well far away from the minimum, less so closer to it.\n",
        "\n",
        "  ANSWER: True"
      ],
      "metadata": {
        "id": "YUsS0SeNG0kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## quiz 8\n",
        "1. [T/F] A strictly convex function $\\mathbb{R}^{d}\\rightarrow \\mathbb{R}$\n",
        "has always a minimizer $x$ on $\\mathbb{R}$.\n",
        "\n",
        "ANSWER: FALSE - similar to a question from last quiz, we can have that the function goes to infinity. For example $\\sum_{i=1}^d e^{x_i}$\n",
        "\n",
        "---\n",
        "\n",
        "2. [T/F] Let $u\\in \\mathbb{R}^{d}$, $d\\geq 2$. The function $f\\left(\n",
        "x\\right) =\\frac{1}{2}x^{\\top }uu^{\\top }x$ is strictly convex on $\\mathbb{R}^{d}$.\n",
        "\n",
        "ANSWER: FALSE - The hessian at any $x$ is $uu^T$ which is positive semidefinite but not necessarily positive definite\n",
        "\n",
        "---\n",
        "\n",
        "3.  [T/F] The gradient descent method seen in class always always decreases the objective function.\n",
        "\n",
        "ANSWER: True - because in class upto when the quiz was posted, we set $\\alpha_k$ as to minimize (I will get back to you on this one). \n",
        "\n",
        "What about if $f$ is a constant function? what about need for Armijo?\n",
        "\n",
        "--- \n",
        "\n",
        "4. [MC] Consider $f:\\mathbb{R}^{d}\\rightarrow \\mathbb{R}$ a smooth convex\n",
        "function defined on the whole of $\\mathbb{R}^{d}$. \n",
        "\n",
        "The condition $\\nabla f\\left( x\\right) =0$ is: necessary and sufficient to the fact that $x$ is a local minimizer of $f\n",
        "$ on $\\mathbb{R}^{d}$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "5. Consider a matrix $Q$ whose smallest eigenvalue is $0.1$ and whose largest eigenvalue is $10$.\n",
        "\n",
        "With the same notations as those used in class, what is the upper bound on $%\n",
        "E\\left( y_{k+1}\\right) /E\\left( y_{k}\\right) $ implied by Kantorovich's\n",
        "inequality? (truncate your anwer to two decimal places)\n",
        "\n",
        "ANSWER: $(10-0.1)^2 / (10+0.1)^2 = 0.96$"
      ],
      "metadata": {
        "id": "HZt0OACOH97Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz 7\n",
        "\n",
        "1. [T/F] Let $f:\\Omega \\rightarrow R$ be a continuous function, where $%\n",
        "\\Omega \\subseteq R^{d}$, and consider \n",
        "\\begin{equation*}\n",
        "\\min_{x\\in \\Omega }f\\left( x\\right) \n",
        "\\end{equation*}\n",
        "\n",
        "A sufficient condition that a minimum $x^{\\ast }\\in \\Omega $ exists is that $%\n",
        "\\Omega $ is closed.\n",
        "\n",
        "ANSWER:FALSE - $f(x) = -1/x$ on $[0,1]$ the min is unbounded but never acheived because $f(0)$ is undefined\n",
        " \n",
        " ---\n",
        "\n",
        "2. [T/F] Consider $\\Omega =\\left\\{ \\left( x,y\\right) \\in R^{2}:x+y\\leq\n",
        "1\\right\\} $. Then $(-1/2,1/3)$ is a feasible direction for $\\left(\n",
        "1/2,1/2\\right) $.\n",
        "\n",
        "ANSWER: We are on the boundary, $.5+.5 = 1$, TRUE because $x+tu$ is in the set for all $t\\geq 0$\n",
        "\n",
        "---\n",
        "\n",
        "3. [T/F] Let $f:\\Omega \\rightarrow R$ be a $C^{2}$ function, where $\\Omega\n",
        "\\subseteq R^{2}$, and consider \n",
        "\\begin{equation*}\n",
        "\\min_{x\\in \\Omega }f\\left( x\\right) \n",
        "\\end{equation*}\n",
        "\n",
        "Let $x^{\\ast }$ be an interior point of $\\Omega $. Assume $\\nabla f\\left(\n",
        "x^{\\ast }\\right) =0$ and \n",
        "\\begin{equation*}\n",
        "D^{2}f\\left( x^{\\ast }\\right) =%\n",
        "\\begin{pmatrix}\n",
        "1 & 3 \\\\ \n",
        "3 & 2%\n",
        "\\end{pmatrix}%\n",
        ".\n",
        "\\end{equation*} \n",
        "Then $x^{\\ast }$ is a local minimum of $f$.\n",
        "\n",
        "ANSWER: FALSE\n",
        "\n",
        "aside - checking for positive semi-definite \n",
        "\n",
        "def: $\\forall x \\quad x^TQx \\geq 0$ but this can be hard to prove directly. What we can do instead is prove using a basis for $x$. Or, especial for the 2d case, we can just compute the eigenvalues. If they are all non-negative then we know that the matrix is positive semi-definite.\n",
        "\n",
        "So in this example, the trace is 3, and the determinant is -7. Thus our eigenvalues satisfy: $\\lambda_1 + \\lambda_2 = 3$ and $\\lambda_1\\lambda_2 = -7$. Clearly from the last equation we know that there must be negative eigenvalue.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "kpz-zJbGVt-N"
      }
    }
  ]
}